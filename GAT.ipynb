{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy/qKe/dgKr4ZimX67eKAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamtakurrey123/Pytorch-Tutorial/blob/main/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBBxqMdVjU1V",
        "outputId": "13311eac-c71e-4023-c42f-d2c6c03ad6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCyUowdyidLa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from layers import GraphAttentionLayer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "PsgVuzv5jevD",
        "outputId": "75a3b68d-6365-40ea-ec74-b15b1fba9f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1b9928c41ade>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphAttentionLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "###    GAT NETWORK MODULE    ###\n",
        "################################\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network (GAT) as described in the paper `\"Graph Attention Networks\" <https://arxiv.org/pdf/1710.10903.pdf>`.\n",
        "    Consists of a 2-layer stack of Graph Attention Layers (GATs). The fist GAT Layer is followed by an ELU activation.\n",
        "    And the second (final) layer is a GAT layer with a single attention head and softmax activation function.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "q6OCjqGVjoZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def __init__(self,\n",
        "        in_features,\n",
        "        n_hidden,\n",
        "        n_heads,\n",
        "        num_classes,\n",
        "        concat=False,\n",
        "        dropout=0.4,\n",
        "        leaky_relu_slope=0.2):\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zVlBvLbNlF61",
        "outputId": "26d4dba5-d2b7-4b5f-9917-bedffb366511"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-24-cfcd27a72165>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-cfcd27a72165>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   \"\"\" Initializes the GAT model.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): number of input features per node.\n",
        "            n_hidden (int): output size of the first Graph Attention Layer.\n",
        "            n_heads (int): number of attention heads in the first Graph Attention Layer.\n",
        "            num_classes (int): number of classes to predict for each node.\n",
        "            concat (bool, optional): Wether to concatinate attention heads or take an average over them for the\n",
        "                output of the first Graph Attention Layer. Defaults to False.\n",
        "            dropout (float, optional): dropout rate. Defaults to 0.4.\n",
        "            leaky_relu_slope (float, optional): alpha (slope) of the leaky relu activation. Defaults to 0.2.\n",
        "        \"\"\"\n",
        "       super(GAT, self).__init__()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6bqW1V2lllbw",
        "outputId": "85d74eaf-eecd-44ea-9eed-e08fd0ad8d8e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-25-3684cb800a23>, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-3684cb800a23>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    super(GAT, self).__init__()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Define the Graph Attention layers\n",
        "        self.gat1 = GraphAttentionLayer(\n",
        "            in_features=in_features, out_features=n_hidden, n_heads=n_heads,\n",
        "            concat=concat, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )\n",
        "\n",
        "        self.gat2 = GraphAttentionLayer(\n",
        "            in_features=n_hidden, out_features=num_classes, n_heads=1,\n",
        "            concat=False, dropout=dropout, leaky_relu_slope=leaky_relu_slope\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "y0l5GpgfqJ1T",
        "outputId": "6e6c0dd8-5d86-4a05-aa0e-428e242b9e41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-26-490bb59533e6>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-490bb59533e6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    self.gat1 = GraphAttentionLayer(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input_tensor: torch.Tensor , adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_tensor (torch.Tensor): Input tensor representing node features.\n",
        "            adj_mat (torch.Tensor): Adjacency matrix representing graph structure.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after the forward pass.\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "ikh_yKfUqP2S"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Apply the first Graph Attention layer\n",
        "        x = self.gat1(input_tensor, adj_mat)\n",
        "        x = F.elu(x) # Apply ELU activation function to the output of the first layer\n",
        "\n",
        "        # Apply the second Graph Attention layer\n",
        "        x = self.gat2(x, adj_mat)\n",
        "\n",
        "        return F.log_softmax(x, dim=1) # Apply log softmax activation function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ymT3UoLyqYBq",
        "outputId": "bcd80876-7aef-4616-848f-d8a28b03a42a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-28-7f08690e1703>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-7f08690e1703>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    x = self.gat1(input_tensor, adj_mat)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}